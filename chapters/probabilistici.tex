% !TeX spellcheck = it_IT

\section{Algoritmi probabilistici}

Cambia la \textbf{nozione di algoritmo}, oltre all'input del problema l'algoritmo è in grado di leggere da una \textbf{sorgente di bit casuali} (0 o 1 con pari probabilità). Sostanzialmente, una DTM con accesso anche ad un nastro casuale utilizzabile quando necessario.\\

L'\textbf{output} quindi non è più deterministico, ma una \textbf{distribuzione di probabilità che dipende dall'input}. Anche il \textbf{tempo di esecuzione} diventa una \textbf{distribuzione} in base all'input.\\

Aggiungiamo un \textbf{componente probabilistico} agli algoritmi. L'output è diventa completamente deterministico se potessimo fissare i bit generati dalla componente casuale.\\

Nessun computer è in grado di simulare veramente una sorgente random, in quanto macchine deterministiche. Senza congegni strani ci si accontenta di \textbf{generatori pseudo-casuali PRNG}, ovvero una sequenza che sembra casuale ma che parte da un seed, e quindi riproducibile. Creano sequenze deterministiche che \textit{sembrano} casuali.\\

\newpage

\subsection{Problema del Taglio Minimo Globale (MinCut)}

Definizione:
\begin{itemize}
	\item \textbf{Input}: un grafo non orientato $G = (V,E)$
	\item \textbf{Soluzione ammissibile}: insieme di vertici $X \subseteq V$, con $X \neq \emptyset$ e che non deve avere complemento vuoto $X^C \neq \emptyset$ (dividere i vertici in due gruppi)
	\item \textbf{Funzione obiettivo}: l'insieme dei lati che hanno un'estremità in $X$ ed una estremità nel complemento di $X$
	$$ \{e \in E | e \cap X \neq \emptyset, e \cap X^C \neq \emptyset \} $$
	i.e., lati facente parti del taglio
	\item \textbf{Tipo}: minimizzazione $\min$
\end{itemize}

Si tratta di un problema $ \in \mathcal{NPO}c$.\\

Un \textbf{taglio} è una partizione dei vertici di un grafo $V$ in due sottoinsiemi disgiunti $S, T$. Gli archi che collegano un vertice di $S$ ad uno di $T$ formano il taglio. Un taglio è \textbf{minimo} se il suo peso è minimo rispetto a tutti gli altri possibili tagli (insieme di lati più piccolo possibile).\\

\paragraph{Lemma:} Il taglio minimo è $\leq$ grado minimo.\\

Il taglio può essere al minimo 1, nel caso di un ponte tra due gruppi di nodi. Al massimo può essere il grado minimo in quanto basta tagliare quel numero di lati per dividere un nodo da tutti gli altri.\\

\newpage

\subsubsection{Algoritmo di Karger}

\paragraph{Contrazione di un grafo su un lato $e$:} Partendo da un grafo sul quale è presente un lato $e$ legato ai vertici $u$ e $v$, una contrazione si ottiene rimuovendo $e$ e facendo coincidere $u$ e $v$. \\

Da notare che se un terzo vertice $x$ è collegato sia a $u$ che a $v$, allora bisogna collegare con due archi il vertice risultato della contrazione $\{u,v\}$ a $x$. Contrarre un lato che ha dei "paralleli" vuol dire contrarre anche quest'ultimi.\\

Indichiamo la contrazione di $e$ su $G$ con $G \downarrow e$.\\

\textbf{Passaggi} dell'algoritmo:
\begin{itemize}
	\item Se $G$ non è connesso, emetti una componente connessa qualunque
	\item Altrimenti, finché ci sono più di 2 vertici, scegli un \textbf{lato casuale} e \textbf{contrai}
	\item Emetti la \textbf{classe di equivalenza} di uno dei due vertici
\end{itemize}

I due gruppi risultanti dal taglio corrisponderanno ai due gruppi di vertici "compressi" ed i lati da tagliare saranno i vertici presenti tra quei gruppi di nodi.\\

\newpage

\paragraph{Esempio:} Grafo $G$
\begin{center}
	\begin{tikzpicture}[scale=0.45]
		\node[draw, circle, fill=black, label=90:1] (1) at (0,0) {};
		\node[draw, circle, fill=black, label=180:2] (2) at (-1.5,-2) {};
		\node[draw, circle, fill=black, label=90:3] (3) at (1.5,-2) {};
		\node[draw, circle, fill=black, label=270:4] (4) at (0,-4) {};
		\node[draw, circle, fill=black, label=90:5] (5) at (3,0) {};
		\node[draw, circle, fill=black, label=270:6] (6) at (3,-4) {};
		\node[draw, circle, fill=black, label=0:7] (7) at (4.5,-2) {};
		
		\draw[-] (1) to node {} (2);
		\draw[-] (1) to node {} (3);
		\draw[-] (2) to node {} (3);
		\draw[-] (2) to node {} (4);
		\draw[-] (3) to node {} (4);
		\draw[-] (3) to node {} (5);
		\draw[-] (3) to node {} (6);
		\draw[-] (3) to node {} (7);
		\draw[-] (5) to node {} (7);
		\draw[-] (6) to node {} (7);
		
	\end{tikzpicture}

	\begin{tikzpicture}[scale=0.45]
		\node[draw, circle, fill=black, label=180:{$\{1,2\}$}] (2) at (-1.5,-2) {};
		\node[draw, circle, fill=black, label=90:3] (3) at (1.5,-2) {};
		\node[draw, circle, fill=black, label=270:4] (4) at (0,-4) {};
		\node[draw, circle, fill=black, label=90:5] (5) at (3,0) {};
		\node[draw, circle, fill=black, label=270:6] (6) at (3,-4) {};
		\node[draw, circle, fill=black, label=0:7] (7) at (4.5,-2) {};
		
		\draw[-, bend left] (2) to node {} (3);
		\draw[-] (2) to node {} (3);
		\draw[-] (2) to node {} (4);
		\draw[-] (3) to node {} (4);
		\draw[-] (3) to node {} (5);
		\draw[-] (3) to node {} (6);
		\draw[-] (3) to node {} (7);
		\draw[-] (5) to node {} (7);
		\draw[-] (6) to node {} (7);
		
	\end{tikzpicture}
	
	\begin{tikzpicture}[scale=0.45]
		\node[draw, circle, fill=black, label=180:{$\{1,2\}$}] (2) at (-1.5,-2) {};
		\node[draw, circle, fill=black, label=90:3] (3) at (1.5,-2) {};
		\node[draw, circle, fill=black, label=270:4] (4) at (0,-4) {};
		\node[draw, circle, fill=black, label=90:5] (5) at (3,0) {};
		\node[draw, circle, fill=black, label=0:{$\{6,7\}$}] (7) at (4.5,-2) {};
		
		\draw[-, bend left] (2) to node {} (3);
		\draw[-] (2) to node {} (3);
		\draw[-] (2) to node {} (4);
		\draw[-] (3) to node {} (4);
		\draw[-] (3) to node {} (5);
		\draw[-, bend right] (3) to node {} (7);
		\draw[-] (3) to node {} (7);
		\draw[-] (5) to node {} (7);
		
	\end{tikzpicture}
	
	\begin{tikzpicture}[scale=0.45]
		\node[draw, circle, fill=black, label=180:{$\{1,2\}$}] (2) at (-1.5,-2) {};
		\node[draw, circle, fill=black, label=0:{$\{3,6,7\}$}] (3) at (1.5,-2) {};
		\node[draw, circle, fill=black, label=270:4] (4) at (0,-4) {};
		\node[draw, circle, fill=black, label=90:5] (5) at (3,0) {};
		
		\draw[-, bend left] (2) to node {} (3);
		\draw[-] (2) to node {} (3);
		\draw[-] (2) to node {} (4);
		\draw[-] (3) to node {} (4);
		\draw[-] (3) to node {} (5);
		\draw[-, bend left] (5) to node {} (3);
		
	\end{tikzpicture}
	
	\begin{tikzpicture}[scale=0.45]
		\node[draw, circle, fill=black, label=180:{$\{1,2\}$}] (2) at (-1.5,-2) {};
		\node[draw, circle, fill=black, label=0:{$\{3,4,6,7\}$}] (3) at (1.5,-2) {};
		\node[draw, circle, fill=black, label=90:5] (5) at (3,0) {};
		
		\draw[-, bend left] (2) to node {} (3);
		\draw[-] (2) to node {} (3);
		\draw[-, bend left] (3) to node {} (2);
		\draw[-] (3) to node {} (5);
		\draw[-, bend left] (5) to node {} (3);
		
	\end{tikzpicture}
	
	\begin{tikzpicture}[scale=0.45]
		\node[draw, circle, fill=black, label=180:{$\{1,2\}$}] (2) at (-1.5,-2) {};
		\node[draw, circle, fill=black, label=0:{$\{3,4,5,6,7\}$}] (3) at (1.5,-2) {};
		
		\draw[-, bend left] (2) to node {} (3);
		\draw[-] (2) to node {} (3);
		\draw[-, bend left] (3) to node {} (2);
		
	\end{tikzpicture}
\end{center}

Quindi possiamo dividere i vertici in $X = \{1,2\}$ e $X^C = \{3,4,5,6,7\}$ tagliando, nel grafo originale i lati $(1,3), (2,3),(2,4)$.

\newpage

Questo algoritmo \textbf{può ottenere la soluzione ottima}, ma anche \textbf{soluzioni arbitrariamente brutte}, non è un algoritmo di approssimazione, è \textbf{probabilistico}, potrebbe trovare qualcosa di molto lontano dall'ottimo, l'importante è che ci sia anche una probabilità di trovare quest'ultimo. Dobbiamo dimostrare che è presente una probabilità positiva di ottenere l'ottimo.\\

\begin{proof}
	Chiamiamo il grafo iniziale $G_1$ e $G_i$ il grafo prima dell'$i$-esima iterazione
	$$ G = G_1 \xrightarrow{G_1 \downarrow e_1} G_2 \xrightarrow{G_2 \downarrow e_2} \, \dots $$
	
	Chiamiamo $X^\ast$ il taglio minimo e $k^\ast$ la dimensione di quest'ultimo.\\
	
	Considerazioni:
	\begin{enumerate}
		\item $G_i$ ha $n-i+1$ vertici (contrarre riduce i vertici sempre di 1)
		\item $G_i$ ha al $\leq m -i+1$ dato che ne cancelliamo almeno 1 (potrebbero esserci paralleli) ogni volta
		\item Ogni taglio di $G_i$ corrisponde ad un taglio di $G$ con la stessa dimensione
		\item Quindi il grado minimo di $G_i$ è $\geq k^\ast$, altrimenti (come visto nel Lemma) il taglio minimo non sarebbe davvero minimo
		\item Se $m_i$ sono i lati di $G_i$
		$$ 2 m_i = \sum_{v \in V_{G_i}} d_{G_i} (v)$$
		sarà pari alla metà della somma dei gradi di tutti i vertici, che a sua volta (per l'osservazione precedente)
		$$ \implies 2m_i \geq k^\ast (n-i+1) \implies m_i \geq \frac{k^\ast (n-i+1)}{2} $$
	\end{enumerate}
	
	Chiamando l'evento $E_i =$ "all'$i$-esima contrazione non contraiamo uno dei lati tagliati dal taglio minimo" (quindi dalla soluzione ottima).\\
	
	\newpage
	
	\paragraph{Lemma: }
	$$P[E_i | E_1, \, \dots \, , E_{i-1}] \geq \frac{n-i-1}{n-i+1}$$
	
	Dove la prima parte rappresenta la probabilità di togliere all'$i$-esimo passaggio uno dei lati necessari per il taglio minimo, dopo aver avuto la fortuna che ciò non sia accaduto per i primi $i-1$ passi.\\
	
	Di conseguenza: 
	\begin{flalign*}
		P[E_i | E_1, \, \dots \, , E_{i-1}]  
		& = 1 - P[\overline{E}_i | E_1, \, \dots \, , E_{i-1}]  \\
		& = 1 - \frac{k^\ast}{m_i}  \\
		& \geq 1 - \frac{k^\ast \cdot 2}{k^\ast (n-i+1)} = \frac{n-i-1}{n-i+1}  \\
		& = \frac{n-i-1}{n-i+1}
	\end{flalign*}
	Quindi questa è la probabilità che all'$i$-esimo passo io non ho mai escluso nessuno dei lati necessari per il taglio minimo.\\
\end{proof}

\newpage

\addcontentsline{toc}{subsubsection}{\protect\numberline{}Teorema}
\paragraph{Teorema:} L'algoritmo di Karger emette il \textbf{taglio minimo} con \textbf{probabilità} 
$$\geq \frac{1}{\left(\begin{array}{c} n \\ 2 \end{array}\right)}$$

\begin{proof} Sapendo che 
	$$ P[E_1 \wedge E_2 \wedge \, \dots \, \wedge E_{n-1}] 
	= P[E_1] \cdot P[E_2 | E_1] \cdot \, \dots \, \cdot P [E_{n-2} | E_1, \, \dots \, , E_{n-3}]
	$$
	Per la proprietà della catena di eventi. Ma ognuna di quelle probabilità è nota, per il Lemma:
	\begin{flalign*}
		\geq \frac{n-2}{n} \cdot \frac{n-3}{n-1} \cdot \frac{n-4}{n - 2} \cdot \, \dots \, \cdot \frac{1}{3} 
		& = \frac{(n-2)! 2}{n!} \\ 
		& = \frac{2}{n(n-1)} \\ 
		& = \frac{1}{\left(\begin{array}{c} n \\ 2 \end{array}\right)} \\
	\end{flalign*}
\end{proof}

Quindi una probabilità relativamente piccola, che scala molto in fretta con $n$, ma \textbf{positiva}.\\

\newpage

\paragraph{Corollario:} Eseguendo l'algoritmo di Karger $\left(\begin{array}{c} n \\ 2 \end{array}\right) \ln n$ volte si ottiene il taglio minimo con probabilità $\geq 1 - \frac{1}{n}$ (possiamo dire "quasi certo", dato che tende ad $1$ per $n \rightarrow +\infty$).\\

\begin{proof}
	Dimostrazione: Sapendo che $\forall x >1$
	$$ \frac{1}{4} \leq1 - \left( 1 - \frac{1}{x}\right)^x \leq \frac{1}{e}$$
	
	Qual'è la probabilità di NON trovare mai l'ottimo? Sarà
	$$ \leq \left(1 - \frac{1}{\left(\begin{array}{c} n \\ 2 \end{array}\right)}\right)^{\left(\begin{array}{c} n \\ 2 \end{array}\right) \ln n} 
	\leq \left(\frac{1}{e}\right)^{\ln n} 
	= \frac{1}{n}
	$$
\end{proof}

Quindi, eseguendo un algoritmo probabilistico che ha tempo polinomiale un numero polinomiale di volte ho sempre tempo polinomiale come risultato, ma sono \textit{quasi} certo di ottenere l'ottimo.\\

Se eseguo tante volte un algoritmo che può darmi soluzioni molto brutte, ma potenzialmente anche l'ottimo, prima o poi qualcosa di buono lo trovo.\\

% End L14

